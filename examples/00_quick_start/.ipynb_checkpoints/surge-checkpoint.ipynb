{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7ce4b7-18d0-49e1-98db-cb0f39faa0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "from sequential_base_model import SequentialBaseModel\n",
    "from rnn_cell_implement import VecAttGRUCell\n",
    "from rnn_dien import dynamic_rnn as dynamic_rnn_dien\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c9c985-2874-4c43-b9dc-363a24365ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__all__ = [\"SURGEModel\"]\n",
    "\n",
    "\n",
    "class SURGEModel(SequentialBaseModel):\n",
    "\n",
    "    def __init__(self, hparams, iterator_creator, seed=None):\n",
    "        \"\"\"Initialization of variables or temp hyperparameters\n",
    "\n",
    "        Args:\n",
    "            hparams (obj): A tf.contrib.training.HParams object, hold the entire set of hyperparameters.\n",
    "            iterator_creator (obj): An iterator to load the data.\n",
    "        \"\"\"\n",
    "        self.hparams = hparams\n",
    "        self.relative_threshold = 0.5 \n",
    "        self.metric_heads = 1\n",
    "        self.attention_heads = 1\n",
    "        self.pool_layers = 1\n",
    "        self.layer_shared = True\n",
    "        if 'kwai' in socket.gethostname():\n",
    "            self.pool_length = 150 # kuaishou\n",
    "        else:\n",
    "            self.pool_length = 30 # taobao\n",
    "        super().__init__(hparams, iterator_creator, seed=None)\n",
    "\n",
    "\n",
    "    def _build_seq_graph(self):\n",
    "        \"\"\" SURGE Model: \n",
    "\n",
    "            1) Interest graph: Graph construction based on metric learning\n",
    "            2) Interest fusion and extraction : Graph convolution and graph pooling \n",
    "            3) Prediction: Flatten pooled graph to reduced sequence\n",
    "        \"\"\"\n",
    "        X = tf.concat(\n",
    "            [self.item_history_embedding, self.cate_history_embedding], 2\n",
    "        )\n",
    "        self.mask = self.iterator.mask\n",
    "        self.float_mask = tf.cast(self.mask, tf.float32)\n",
    "        self.real_sequence_length = tf.reduce_sum(self.mask, 1)\n",
    "\n",
    "        with tf.name_scope('interest_graph'):\n",
    "            ## Node similarity metric learning \n",
    "            S = []\n",
    "            for i in range(self.metric_heads):\n",
    "                # weighted cosine similarity\n",
    "                self.weighted_tensor = tf.layers.dense(tf.ones([1, 1]), X.shape.as_list()[-1], use_bias=False)\n",
    "                X_fts = X * tf.expand_dims(self.weighted_tensor, 0)\n",
    "                X_fts = tf.nn.l2_normalize(X_fts,dim=2)\n",
    "                S_one = tf.matmul(X_fts, tf.transpose(X_fts, (0,2,1))) # B*L*L\n",
    "                # min-max normalization for mask\n",
    "                S_min = tf.reduce_min(S_one, -1, keepdims=True)\n",
    "                S_max = tf.reduce_max(S_one, -1, keepdims=True)\n",
    "                S_one = (S_one - S_min) / (S_max - S_min)\n",
    "                S += [S_one]\n",
    "            S = tf.reduce_mean(tf.stack(S, 0), 0)\n",
    "            # mask invalid nodes\n",
    "            S = S * tf.expand_dims(self.float_mask, -1) * tf.expand_dims(self.float_mask, -2)\n",
    "\n",
    "            ## Graph sparsification via seted sparseness \n",
    "            S_flatten = tf.reshape(S, [tf.shape(S)[0],-1])\n",
    "            if 'kwai' in socket.gethostname():\n",
    "                sorted_S_flatten = tf.contrib.framework.sort(S_flatten, direction='DESCENDING', axis=-1) # B*L -> B*L\n",
    "            else:\n",
    "                sorted_S_flatten = tf.sort(S_flatten, direction='DESCENDING', axis=-1) # B*L -> B*L\n",
    "            # relative ranking strategy of the entire graph\n",
    "            num_edges = tf.cast(tf.count_nonzero(S, [1,2]), tf.float32) # B\n",
    "            to_keep_edge = tf.cast(tf.math.ceil(num_edges * self.relative_threshold), tf.int32)\n",
    "            if 'kwai' in socket.gethostname():\n",
    "                threshold_index = tf.stack([tf.range(tf.shape(X)[0]), tf.cast(to_keep_edge, tf.int32)], 1) # B*2\n",
    "                threshold_score = tf.gather_nd(sorted_S_flatten, threshold_index) # indices[:-1]=(B) + data[indices[-1]=() --> (B)\n",
    "            else:\n",
    "                threshold_score = tf.gather_nd(sorted_S_flatten, tf.expand_dims(tf.cast(to_keep_edge, tf.int32), -1), batch_dims=1) # indices[:-1]=(B) + data[indices[-1]=() --> (B)\n",
    "            A = tf.cast(tf.greater(S, tf.expand_dims(tf.expand_dims(threshold_score, -1), -1)), tf.float32)\n",
    "\n",
    "\n",
    "        with tf.name_scope('interest_fusion_extraction'):\n",
    "            for l in range(self.pool_layers):\n",
    "                reuse = False if l==0 else True\n",
    "                X, A, graph_readout, alphas = self._interest_fusion_extraction(X, A, layer=l, reuse=reuse)\n",
    "\n",
    "\n",
    "        with tf.name_scope('prediction'):\n",
    "            # flatten pooled graph to reduced sequence \n",
    "            output_shape = self.mask.get_shape()\n",
    "            if 'kwai' in socket.gethostname():\n",
    "                sorted_mask_index = tf.contrib.framework.argsort(self.mask, direction='DESCENDING', stable=True, axis=-1) # B*L -> B*L\n",
    "                sorted_mask = tf.contrib.framework.sort(self.mask, direction='DESCENDING', axis=-1) # B*L -> B*L\n",
    "            else:\n",
    "                sorted_mask_index = tf.argsort(self.mask, direction='DESCENDING', stable=True, axis=-1) # B*L -> B*L\n",
    "                sorted_mask = tf.sort(self.mask, direction='DESCENDING', axis=-1) # B*L -> B*L\n",
    "            sorted_mask.set_shape(output_shape)\n",
    "            sorted_mask_index.set_shape(output_shape)\n",
    "            X = tf.batch_gather(X, sorted_mask_index) # B*L*F  < B*L = B*L*F\n",
    "            self.mask = sorted_mask\n",
    "            self.reduced_sequence_length = tf.reduce_sum(self.mask, 1) # B\n",
    "\n",
    "            # cut useless sequence tail per batch \n",
    "            self.to_max_length = tf.range(tf.reduce_max(self.reduced_sequence_length)) # l\n",
    "            X = tf.gather(X, self.to_max_length, axis=1) # B*L*F -> B*l*F\n",
    "            self.mask = tf.gather(self.mask, self.to_max_length, axis=1) # B*L -> B*l\n",
    "            self.reduced_sequence_length = tf.reduce_sum(self.mask, 1) # B\n",
    "\n",
    "            # use cluster score as attention weights in AUGRU \n",
    "            _, alphas = self._attention_fcn(self.target_item_embedding, X, 'AGRU', False, return_alpha=True)\n",
    "            _, final_state = dynamic_rnn_dien(\n",
    "                VecAttGRUCell(self.hparams.hidden_size),\n",
    "                inputs=X,\n",
    "                att_scores = tf.expand_dims(alphas, -1),\n",
    "                sequence_length=self.reduced_sequence_length,\n",
    "                dtype=tf.float32,\n",
    "                scope=\"gru\"\n",
    "            )\n",
    "            model_output = tf.concat([final_state, graph_readout, self.target_item_embedding, graph_readout*self.target_item_embedding], 1)\n",
    "\n",
    "        return model_output\n",
    "\n",
    "  \n",
    "    def _attention_fcn(self, query, key_value, name, reuse, return_alpha=False):\n",
    "        \"\"\"Apply attention by fully connected layers.\n",
    "\n",
    "        Args:\n",
    "            query (obj): The embedding of target item or cluster which is regarded as a query in attention operations.\n",
    "            key_value (obj): The embedding of history items which is regarded as keys or values in attention operations.\n",
    "            name (obj): The name of variable W \n",
    "            reuse (obj): Reusing variable W in query operation \n",
    "            return_alpha (obj): Returning attention weights\n",
    "\n",
    "        Returns:\n",
    "            output (obj): Weighted sum of value embedding.\n",
    "            att_weights (obj):  Attention weights\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"attention_fcn\"+str(name), reuse=reuse):\n",
    "            query_size = query.shape[-1].value\n",
    "            boolean_mask = tf.equal(self.mask, tf.ones_like(self.mask))\n",
    "\n",
    "            attention_mat = tf.get_variable(\n",
    "                name=\"attention_mat\"+str(name),\n",
    "                shape=[key_value.shape.as_list()[-1], query_size],\n",
    "                initializer=self.initializer,\n",
    "            )\n",
    "            att_inputs = tf.tensordot(key_value, attention_mat, [[2], [0]])\n",
    "\n",
    "            if query.shape.ndims != att_inputs.shape.ndims:\n",
    "                queries = tf.reshape(\n",
    "                    tf.tile(query, [1, tf.shape(att_inputs)[1]]), tf.shape(att_inputs)\n",
    "                )\n",
    "            else:\n",
    "                queries = query\n",
    "\n",
    "            last_hidden_nn_layer = tf.concat(\n",
    "                [att_inputs, queries, att_inputs - queries, att_inputs * queries], -1\n",
    "            )\n",
    "            att_fnc_output = self._fcn_net(\n",
    "                last_hidden_nn_layer, self.hparams.att_fcn_layer_sizes, scope=\"att_fcn\"\n",
    "            )\n",
    "            att_fnc_output = tf.squeeze(att_fnc_output, -1)\n",
    "            mask_paddings = tf.ones_like(att_fnc_output) * (-(2 ** 32) + 1)\n",
    "            att_weights = tf.nn.softmax(\n",
    "                tf.where(boolean_mask, att_fnc_output, mask_paddings),\n",
    "                name=\"att_weights\",\n",
    "            )\n",
    "            output = key_value * tf.expand_dims(att_weights, -1)\n",
    "            if not return_alpha:\n",
    "                return output\n",
    "            else:\n",
    "                return output, att_weights\n",
    "\n",
    "\n",
    "    def _interest_fusion_extraction(self, X, A, layer, reuse):\n",
    "        \"\"\"Interest fusion and extraction via graph convolution and graph pooling \n",
    "\n",
    "        Args:\n",
    "            X (obj): Node embedding of graph\n",
    "            A (obj): Adjacency matrix of graph\n",
    "            layer (obj): Interest fusion and extraction layer\n",
    "            reuse (obj): Reusing variable W in query operation \n",
    "\n",
    "        Returns:\n",
    "            X (obj): Aggerated cluster embedding \n",
    "            A (obj): Pooled adjacency matrix \n",
    "            graph_readout (obj): Readout embedding after graph pooling\n",
    "            cluster_score (obj): Cluster score for AUGRU in prediction layer\n",
    "\n",
    "        \"\"\"\n",
    "        with tf.name_scope('interest_fusion'):\n",
    "            ## cluster embedding\n",
    "            A_bool = tf.cast(tf.greater(A, 0), A.dtype)\n",
    "            A_bool = A_bool * (tf.ones([A.shape.as_list()[1],A.shape.as_list()[1]]) - tf.eye(A.shape.as_list()[1])) + tf.eye(A.shape.as_list()[1])\n",
    "            D = tf.reduce_sum(A_bool, axis=-1) # B*L\n",
    "            D = tf.sqrt(D)[:, None] + K.epsilon() # B*1*L\n",
    "            A = (A_bool / D) / tf.transpose(D, perm=(0,2,1)) # B*L*L / B*1*L / B*L*1\n",
    "            X_q = tf.matmul(A, tf.matmul(A, X)) # B*L*F\n",
    "\n",
    "            Xc = []\n",
    "            for i in range(self.attention_heads):\n",
    "                ## cluster- and query-aware attention\n",
    "                if not self.layer_shared:\n",
    "                    _, f_1 = self._attention_fcn(X_q, X, 'f1_layer_'+str(layer)+'_'+str(i), False, return_alpha=True)\n",
    "                    _, f_2 = self._attention_fcn(self.target_item_embedding, X, 'f2_layer_'+str(layer)+'_'+str(i), False, return_alpha=True)\n",
    "                if self.layer_shared:\n",
    "                    _, f_1 = self._attention_fcn(X_q, X, 'f1_shared'+'_'+str(i), reuse, return_alpha=True)\n",
    "                    _, f_2 = self._attention_fcn(self.target_item_embedding, X, 'f2_shared'+'_'+str(i), reuse, return_alpha=True)\n",
    "\n",
    "                ## graph attentive convolution\n",
    "                E = A_bool * tf.expand_dims(f_1,1) + A_bool * tf.transpose(tf.expand_dims(f_2,1), (0,2,1)) # B*L*1 x B*L*1 -> B*L*L\n",
    "                E = tf.nn.leaky_relu(E)\n",
    "                boolean_mask = tf.equal(A_bool, tf.ones_like(A_bool))\n",
    "                mask_paddings = tf.ones_like(E) * (-(2 ** 32) + 1)\n",
    "                E = tf.nn.softmax(\n",
    "                    tf.where(boolean_mask, E, mask_paddings),\n",
    "                    axis = -1\n",
    "                )\n",
    "                Xc_one = tf.matmul(E, X) # B*L*L x B*L*F -> B*L*F\n",
    "                Xc_one = tf.layers.dense(Xc_one, 40, use_bias=False)\n",
    "                Xc_one += X\n",
    "                Xc += [tf.nn.leaky_relu(Xc_one)]\n",
    "            Xc = tf.reduce_mean(tf.stack(Xc, 0), 0)\n",
    "\n",
    "        with tf.name_scope('interest_extraction'):\n",
    "            ## cluster fitness score \n",
    "            X_q = tf.matmul(A, tf.matmul(A, Xc)) # B*L*F\n",
    "            cluster_score = []\n",
    "            for i in range(self.attention_heads):\n",
    "                if not self.layer_shared:\n",
    "                    _, f_1 = self._attention_fcn(X_q, Xc, 'f1_layer_'+str(layer)+'_'+str(i), True, return_alpha=True)\n",
    "                    _, f_2 = self._attention_fcn(self.target_item_embedding, Xc, 'f2_layer_'+str(layer)+'_'+str(i), True, return_alpha=True)\n",
    "                if self.layer_shared:\n",
    "                    _, f_1 = self._attention_fcn(X_q, Xc, 'f1_shared'+'_'+str(i), True, return_alpha=True)\n",
    "                    _, f_2 = self._attention_fcn(self.target_item_embedding, Xc, 'f2_shared'+'_'+str(i), True, return_alpha=True)\n",
    "                cluster_score += [f_1 + f_2]\n",
    "            cluster_score = tf.reduce_mean(tf.stack(cluster_score, 0), 0)\n",
    "            boolean_mask = tf.equal(self.mask, tf.ones_like(self.mask))\n",
    "            mask_paddings = tf.ones_like(cluster_score) * (-(2 ** 32) + 1)\n",
    "            cluster_score = tf.nn.softmax(\n",
    "                tf.where(boolean_mask, cluster_score, mask_paddings),\n",
    "                axis = -1\n",
    "            )\n",
    "\n",
    "            ## graph pooling\n",
    "            num_nodes = tf.reduce_sum(self.mask, 1) # B\n",
    "            boolean_pool = tf.greater(num_nodes, self.pool_length)\n",
    "            to_keep = tf.where(boolean_pool, \n",
    "                               tf.cast(self.pool_length + (self.real_sequence_length - self.pool_length)/self.pool_layers*(self.pool_layers-layer-1), tf.int32), \n",
    "                               num_nodes)  # B\n",
    "            cluster_score = cluster_score * self.float_mask # B*L\n",
    "            if 'kwai' in socket.gethostname():\n",
    "                sorted_score = tf.contrib.framework.sort(cluster_score, direction='DESCENDING', axis=-1) # B*L\n",
    "                target_index = tf.stack([tf.range(tf.shape(Xc)[0]), tf.cast(to_keep, tf.int32)], 1) # B*2\n",
    "                target_score = tf.gather_nd(sorted_score, target_index) + K.epsilon() # indices[:-1]=(B) + data[indices[-1]=() --> (B)\n",
    "            else:\n",
    "                sorted_score = tf.sort(cluster_score, direction='DESCENDING', axis=-1) # B*L\n",
    "                target_score = tf.gather_nd(sorted_score, tf.expand_dims(tf.cast(to_keep, tf.int32), -1), batch_dims=1) + K.epsilon() # indices[:-1]=(B) + data[indices[-1]=() --> (B)\n",
    "            topk_mask = tf.greater(cluster_score, tf.expand_dims(target_score, -1)) # B*L + B*1 -> B*L\n",
    "            self.mask = tf.cast(topk_mask, tf.int32)\n",
    "            self.float_mask = tf.cast(self.mask, tf.float32)\n",
    "            self.reduced_sequence_length = tf.reduce_sum(self.mask, 1)\n",
    "\n",
    "            ## ensure graph connectivity \n",
    "            E = E * tf.expand_dims(self.float_mask, -1) * tf.expand_dims(self.float_mask, -2)\n",
    "            A = tf.matmul(tf.matmul(E, A_bool),\n",
    "                          tf.transpose(E, (0,2,1))) # B*C*L x B*L*L x B*L*C = B*C*C\n",
    "            ## graph readout \n",
    "            graph_readout = tf.reduce_sum(Xc*tf.expand_dims(cluster_score,-1)*tf.expand_dims(self.float_mask, -1), 1)\n",
    "\n",
    "        return Xc, A, graph_readout, cluster_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119155d0-51c5-4a26-a285-dea851376d79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
