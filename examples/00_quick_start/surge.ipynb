{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7ce4b7-18d0-49e1-98db-cb0f39faa0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "from sequential_base_model import SequentialBaseModel\n",
    "from rnn_cell_implement import VecAttGRUCell\n",
    "from rnn_dien import dynamic_rnn as dynamic_rnn_dien\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c9c985-2874-4c43-b9dc-363a24365ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__all__ = [\"SURGEModel\"]\n",
    "\n",
    "\n",
    "class SURGEModel(SequentialBaseModel):\n",
    "\n",
    "    def __init__(self, hparams, iterator_creator, seed=None):\n",
    "        \"\"\"Initialization of variables or temp hyperparameters\n",
    "\n",
    "        Args:\n",
    "            hparams (obj): A tf.contrib.training.HParams object, hold the entire set of hyperparameters.\n",
    "            iterator_creator (obj): An iterator to load the data.\n",
    "        \"\"\"\n",
    "        self.hparams = hparams\n",
    "        self.relative_threshold = 0.5 \n",
    "        self.metric_heads = 1\n",
    "        self.attention_heads = 1\n",
    "        self.pool_layers = 1\n",
    "        self.layer_shared = True\n",
    "        if 'kwai' in socket.gethostname():\n",
    "            self.pool_length = 150 # kuaishou\n",
    "        else:\n",
    "            self.pool_length = 30 # taobao\n",
    "        super().__init__(hparams, iterator_creator, seed=None)\n",
    "\n",
    "\n",
    "    def _build_seq_graph(self):\n",
    "        \"\"\" SURGE Model: \n",
    "\n",
    "            1) Interest graph: Graph construction based on metric learning\n",
    "            2) Interest fusion and extraction : Graph convolution and graph pooling \n",
    "            3) Prediction: Flatten pooled graph to reduced sequence\n",
    "        \"\"\"\n",
    "        X = tf.concat(\n",
    "            [self.item_history_embedding, self.cate_history_embedding], 2\n",
    "        )\n",
    "        self.mask = self.iterator.mask\n",
    "        self.float_mask = tf.cast(self.mask, tf.float32)\n",
    "        self.real_sequence_length = tf.reduce_sum(self.mask, 1)\n",
    "\n",
    "        with tf.name_scope('interest_graph'):\n",
    "            ## Node similarity metric learning \n",
    "            S = []\n",
    "            for i in range(self.metric_heads):\n",
    "                # weighted cosine similarity\n",
    "                self.weighted_tensor = tf.layers.dense(tf.ones([1, 1]), X.shape.as_list()[-1], use_bias=False)\n",
    "                X_fts = X * tf.expand_dims(self.weighted_tensor, 0)\n",
    "                X_fts = tf.nn.l2_normalize(X_fts,dim=2)\n",
    "                S_one = tf.matmul(X_fts, tf.transpose(X_fts, (0,2,1))) # B*L*L\n",
    "                # min-max normalization for mask\n",
    "                S_min = tf.reduce_min(S_one, -1, keepdims=True)\n",
    "                S_max = tf.reduce_max(S_one, -1, keepdims=True)\n",
    "                S_one = (S_one - S_min) / (S_max - S_min)\n",
    "                S += [S_one]\n",
    "            S = tf.reduce_mean(tf.stack(S, 0), 0)\n",
    "            # mask invalid nodes\n",
    "            S = S * tf.expand_dims(self.float_mask, -1) * tf.expand_dims(self.float_mask, -2)\n",
    "\n",
    "            ## Graph sparsification via seted sparseness \n",
    "            S_flatten = tf.reshape(S, [tf.shape(S)[0],-1])\n",
    "            if 'kwai' in socket.gethostname():\n",
    "                sorted_S_flatten = tf.contrib.framework.sort(S_flatten, direction='DESCENDING', axis=-1) # B*L -> B*L\n",
    "            else:\n",
    "                sorted_S_flatten = tf.sort(S_flatten, direction='DESCENDING', axis=-1) # B*L -> B*L\n",
    "            # relative ranking strategy of the entire graph\n",
    "            num_edges = tf.cast(tf.count_nonzero(S, [1,2]), tf.float32) # B\n",
    "            to_keep_edge = tf.cast(tf.math.ceil(num_edges * self.relative_threshold), tf.int32)\n",
    "            if 'kwai' in socket.gethostname():\n",
    "                threshold_index = tf.stack([tf.range(tf.shape(X)[0]), tf.cast(to_keep_edge, tf.int32)], 1) # B*2\n",
    "                threshold_score = tf.gather_nd(sorted_S_flatten, threshold_index) # indices[:-1]=(B) + data[indices[-1]=() --> (B)\n",
    "            else:\n",
    "                threshold_score = tf.gather_nd(sorted_S_flatten, tf.expand_dims(tf.cast(to_keep_edge, tf.int32), -1), batch_dims=1) # indices[:-1]=(B) + data[indices[-1]=() --> (B)\n",
    "            A = tf.cast(tf.greater(S, tf.expand_dims(tf.expand_dims(threshold_score, -1), -1)), tf.float32)\n",
    "\n",
    "\n",
    "        with tf.name_scope('interest_fusion_extraction'):\n",
    "            for l in range(self.pool_layers):\n",
    "                reuse = False if l==0 else True\n",
    "                X, A, graph_readout, alphas = self._interest_fusion_extraction(X, A, layer=l, reuse=reuse)\n",
    "\n",
    "\n",
    "        with tf.name_scope('prediction'):\n",
    "            # flatten pooled graph to reduced sequence \n",
    "            output_shape = self.mask.get_shape()\n",
    "            if 'kwai' in socket.gethostname():\n",
    "                sorted_mask_index = tf.contrib.framework.argsort(self.mask, direction='DESCENDING', stable=True, axis=-1) # B*L -> B*L\n",
    "                sorted_mask = tf.contrib.framework.sort(self.mask, direction='DESCENDING', axis=-1) # B*L -> B*L\n",
    "            else:\n",
    "                sorted_mask_index = tf.argsort(self.mask, direction='DESCENDING', stable=True, axis=-1) # B*L -> B*L\n",
    "                sorted_mask = tf.sort(self.mask, direction='DESCENDING', axis=-1) # B*L -> B*L\n",
    "            sorted_mask.set_shape(output_shape)\n",
    "            sorted_mask_index.set_shape(output_shape)\n",
    "            X = tf.batch_gather(X, sorted_mask_index) # B*L*F  < B*L = B*L*F\n",
    "            self.mask = sorted_mask\n",
    "            self.reduced_sequence_length = tf.reduce_sum(self.mask, 1) # B\n",
    "\n",
    "            # cut useless sequence tail per batch \n",
    "            self.to_max_length = tf.range(tf.reduce_max(self.reduced_sequence_length)) # l\n",
    "            X = tf.gather(X, self.to_max_length, axis=1) # B*L*F -> B*l*F\n",
    "            self.mask = tf.gather(self.mask, self.to_max_length, axis=1) # B*L -> B*l\n",
    "            self.reduced_sequence_length = tf.reduce_sum(self.mask, 1) # B\n",
    "\n",
    "            # use cluster score as attention weights in AUGRU \n",
    "            _, alphas = self._attention_fcn(self.target_item_embedding, X, 'AGRU', False, return_alpha=True)\n",
    "            _, final_state = dynamic_rnn_dien(\n",
    "                VecAttGRUCell(self.hparams.hidden_size),\n",
    "                inputs=X,\n",
    "                att_scores = tf.expand_dims(alphas, -1),\n",
    "                sequence_length=self.reduced_sequence_length,\n",
    "                dtype=tf.float32,\n",
    "                scope=\"gru\"\n",
    "            )\n",
    "            model_output = tf.concat([final_state, graph_readout, self.target_item_embedding, graph_readout*self.target_item_embedding], 1)\n",
    "\n",
    "        return model_output\n",
    "\n",
    "  \n",
    "    def _attention_fcn(self, query, key_value, name, reuse, return_alpha=False):\n",
    "        \"\"\"Apply attention by fully connected layers.\n",
    "\n",
    "        Args:\n",
    "            query (obj): The embedding of target item or cluster which is regarded as a query in attention operations.\n",
    "            key_value (obj): The embedding of history items which is regarded as keys or values in attention operations.\n",
    "            name (obj): The name of variable W \n",
    "            reuse (obj): Reusing variable W in query operation \n",
    "            return_alpha (obj): Returning attention weights\n",
    "\n",
    "        Returns:\n",
    "            output (obj): Weighted sum of value embedding.\n",
    "            att_weights (obj):  Attention weights\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"attention_fcn\"+str(name), reuse=reuse):\n",
    "            query_size = query.shape[-1].value\n",
    "            boolean_mask = tf.equal(self.mask, tf.ones_like(self.mask))\n",
    "\n",
    "            attention_mat = tf.get_variable(\n",
    "                name=\"attention_mat\"+str(name),\n",
    "                shape=[key_value.shape.as_list()[-1], query_size],\n",
    "                initializer=self.initializer,\n",
    "            )\n",
    "            att_inputs = tf.tensordot(key_value, attention_mat, [[2], [0]])\n",
    "\n",
    "            if query.shape.ndims != att_inputs.shape.ndims:\n",
    "                queries = tf.reshape(\n",
    "                    tf.tile(query, [1, tf.shape(att_inputs)[1]]), tf.shape(att_inputs)\n",
    "                )\n",
    "            else:\n",
    "                queries = query\n",
    "\n",
    "            last_hidden_nn_layer = tf.concat(\n",
    "                [att_inputs, queries, att_inputs - queries, att_inputs * queries], -1\n",
    "            )\n",
    "            att_fnc_output = self._fcn_net(\n",
    "                last_hidden_nn_layer, self.hparams.att_fcn_layer_sizes, scope=\"att_fcn\"\n",
    "            )\n",
    "            att_fnc_output = tf.squeeze(att_fnc_output, -1)\n",
    "            mask_paddings = tf.ones_like(att_fnc_output) * (-(2 ** 32) + 1)\n",
    "            att_weights = tf.nn.softmax(\n",
    "                tf.where(boolean_mask, att_fnc_output, mask_paddings),\n",
    "                name=\"att_weights\",\n",
    "            )\n",
    "            output = key_value * tf.expand_dims(att_weights, -1)\n",
    "            if not return_alpha:\n",
    "                return output\n",
    "            else:\n",
    "                return output, att_weights\n",
    "\n",
    "\n",
    "    def _interest_fusion_extraction(self, X, A, layer, reuse):\n",
    "        \"\"\"Interest fusion and extraction via graph convolution and graph pooling \n",
    "\n",
    "        Args:\n",
    "            X (obj): Node embedding of graph\n",
    "            A (obj): Adjacency matrix of graph\n",
    "            layer (obj): Interest fusion and extraction layer\n",
    "            reuse (obj): Reusing variable W in query operation \n",
    "\n",
    "        Returns:\n",
    "            X (obj): Aggerated cluster embedding \n",
    "            A (obj): Pooled adjacency matrix \n",
    "            graph_readout (obj): Readout embedding after graph pooling\n",
    "            cluster_score (obj): Cluster score for AUGRU in prediction layer\n",
    "\n",
    "        \"\"\"\n",
    "        with tf.name_scope('interest_fusion'):\n",
    "            ## cluster embedding\n",
    "            A_bool = tf.cast(tf.greater(A, 0), A.dtype)\n",
    "            A_bool = A_bool * (tf.ones([A.shape.as_list()[1],A.shape.as_list()[1]]) - tf.eye(A.shape.as_list()[1])) + tf.eye(A.shape.as_list()[1])\n",
    "            D = tf.reduce_sum(A_bool, axis=-1) # B*L\n",
    "            D = tf.sqrt(D)[:, None] + K.epsilon() # B*1*L\n",
    "            A = (A_bool / D) / tf.transpose(D, perm=(0,2,1)) # B*L*L / B*1*L / B*L*1\n",
    "            X_q = tf.matmul(A, tf.matmul(A, X)) # B*L*F\n",
    "\n",
    "            Xc = []\n",
    "            for i in range(self.attention_heads):\n",
    "                ## cluster- and query-aware attention\n",
    "                if not self.layer_shared:\n",
    "                    _, f_1 = self._attention_fcn(X_q, X, 'f1_layer_'+str(layer)+'_'+str(i), False, return_alpha=True)\n",
    "                    _, f_2 = self._attention_fcn(self.target_item_embedding, X, 'f2_layer_'+str(layer)+'_'+str(i), False, return_alpha=True)\n",
    "                if self.layer_shared:\n",
    "                    _, f_1 = self._attention_fcn(X_q, X, 'f1_shared'+'_'+str(i), reuse, return_alpha=True)\n",
    "                    _, f_2 = self._attention_fcn(self.target_item_embedding, X, 'f2_shared'+'_'+str(i), reuse, return_alpha=True)\n",
    "\n",
    "                ## graph attentive convolution\n",
    "                E = A_bool * tf.expand_dims(f_1,1) + A_bool * tf.transpose(tf.expand_dims(f_2,1), (0,2,1)) # B*L*1 x B*L*1 -> B*L*L\n",
    "                E = tf.nn.leaky_relu(E)\n",
    "                boolean_mask = tf.equal(A_bool, tf.ones_like(A_bool))\n",
    "                mask_paddings = tf.ones_like(E) * (-(2 ** 32) + 1)\n",
    "                E = tf.nn.softmax(\n",
    "                    tf.where(boolean_mask, E, mask_paddings),\n",
    "                    axis = -1\n",
    "                )\n",
    "                Xc_one = tf.matmul(E, X) # B*L*L x B*L*F -> B*L*F\n",
    "                Xc_one = tf.layers.dense(Xc_one, 40, use_bias=False)\n",
    "                Xc_one += X\n",
    "                Xc += [tf.nn.leaky_relu(Xc_one)]\n",
    "            Xc = tf.reduce_mean(tf.stack(Xc, 0), 0)\n",
    "\n",
    "        with tf.name_scope('interest_extraction'):\n",
    "            ## cluster fitness score \n",
    "            X_q = tf.matmul(A, tf.matmul(A, Xc)) # B*L*F\n",
    "            cluster_score = []\n",
    "            for i in range(self.attention_heads):\n",
    "                if not self.layer_shared:\n",
    "                    _, f_1 = self._attention_fcn(X_q, Xc, 'f1_layer_'+str(layer)+'_'+str(i), True, return_alpha=True)\n",
    "                    _, f_2 = self._attention_fcn(self.target_item_embedding, Xc, 'f2_layer_'+str(layer)+'_'+str(i), True, return_alpha=True)\n",
    "                if self.layer_shared:\n",
    "                    _, f_1 = self._attention_fcn(X_q, Xc, 'f1_shared'+'_'+str(i), True, return_alpha=True)\n",
    "                    _, f_2 = self._attention_fcn(self.target_item_embedding, Xc, 'f2_shared'+'_'+str(i), True, return_alpha=True)\n",
    "                cluster_score += [f_1 + f_2]\n",
    "            cluster_score = tf.reduce_mean(tf.stack(cluster_score, 0), 0)\n",
    "            boolean_mask = tf.equal(self.mask, tf.ones_like(self.mask))\n",
    "            mask_paddings = tf.ones_like(cluster_score) * (-(2 ** 32) + 1)\n",
    "            cluster_score = tf.nn.softmax(\n",
    "                tf.where(boolean_mask, cluster_score, mask_paddings),\n",
    "                axis = -1\n",
    "            )\n",
    "\n",
    "            ## graph pooling\n",
    "            num_nodes = tf.reduce_sum(self.mask, 1) # B\n",
    "            boolean_pool = tf.greater(num_nodes, self.pool_length)\n",
    "            to_keep = tf.where(boolean_pool, \n",
    "                               tf.cast(self.pool_length + (self.real_sequence_length - self.pool_length)/self.pool_layers*(self.pool_layers-layer-1), tf.int32), \n",
    "                               num_nodes)  # B\n",
    "            cluster_score = cluster_score * self.float_mask # B*L\n",
    "            if 'kwai' in socket.gethostname():\n",
    "                sorted_score = tf.contrib.framework.sort(cluster_score, direction='DESCENDING', axis=-1) # B*L\n",
    "                target_index = tf.stack([tf.range(tf.shape(Xc)[0]), tf.cast(to_keep, tf.int32)], 1) # B*2\n",
    "                target_score = tf.gather_nd(sorted_score, target_index) + K.epsilon() # indices[:-1]=(B) + data[indices[-1]=() --> (B)\n",
    "            else:\n",
    "                sorted_score = tf.sort(cluster_score, direction='DESCENDING', axis=-1) # B*L\n",
    "                target_score = tf.gather_nd(sorted_score, tf.expand_dims(tf.cast(to_keep, tf.int32), -1), batch_dims=1) + K.epsilon() # indices[:-1]=(B) + data[indices[-1]=() --> (B)\n",
    "            topk_mask = tf.greater(cluster_score, tf.expand_dims(target_score, -1)) # B*L + B*1 -> B*L\n",
    "            self.mask = tf.cast(topk_mask, tf.int32)\n",
    "            self.float_mask = tf.cast(self.mask, tf.float32)\n",
    "            self.reduced_sequence_length = tf.reduce_sum(self.mask, 1)\n",
    "\n",
    "            ## ensure graph connectivity \n",
    "            E = E * tf.expand_dims(self.float_mask, -1) * tf.expand_dims(self.float_mask, -2)\n",
    "            A = tf.matmul(tf.matmul(E, A_bool),\n",
    "                          tf.transpose(E, (0,2,1))) # B*C*L x B*L*L x B*L*C = B*C*C\n",
    "            ## graph readout \n",
    "            graph_readout = tf.reduce_sum(Xc*tf.expand_dims(cluster_score,-1)*tf.expand_dims(self.float_mask, -1), 1)\n",
    "\n",
    "        return Xc, A, graph_readout, cluster_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e51e19a-54eb-4893-a576-25f1b65acd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "#pylint: disable=no-member\n",
    "#pylint: disable=no-name-in-module\n",
    "#pylint: disable=import-error\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import socket\n",
    "import getpass\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "import setproctitle\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from reco_utils.common.constants import SEED\n",
    "from deeprec_utils import (\n",
    "    prepare_hparams\n",
    ")\n",
    "from reco_utils.dataset.sequential_reviews import data_preprocessing, strong_data_preprocessing\n",
    "from reco_utils.dataset.sequential_reviews import group_sequence\n",
    "\n",
    "\n",
    "from reco_utils.recommender.deeprec.io.sequential_iterator import (\n",
    "    SequentialIterator,\n",
    "    SASequentialIterator,\n",
    "    RecentSASequentialIterator,\n",
    "    ShuffleSASequentialIterator\n",
    ")\n",
    "\n",
    "from reco_utils.common.visdom_utils import VizManager\n",
    "from tensorboardX import SummaryWriter\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6058d2-822d-479c-9f21-f3f407601256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagValues at 0x108937a10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "119155d0-51c5-4a26-a285-dea851376d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL Flags parsing error: Unknown command line flag 'f'\n",
      "Pass --helpshort or --helpfull to see help on flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/absl/app.py:156\u001b[0m, in \u001b[0;36mparse_flags_with_usage\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m FLAGS(args)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m flags\u001b[38;5;241m.\u001b[39mError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/absl/flags/_flagvalues.py:685\u001b[0m, in \u001b[0;36mFlagValues.__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    684\u001b[0m   suggestions \u001b[38;5;241m=\u001b[39m _helpers\u001b[38;5;241m.\u001b[39mget_flag_suggestions(name, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m--> 685\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _exceptions\u001b[38;5;241m.\u001b[39mUnrecognizedFlagError(\n\u001b[1;32m    686\u001b[0m       name, value, suggestions\u001b[38;5;241m=\u001b[39msuggestions)\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmark_as_parsed()\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 670\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 670\u001b[0m     app\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/absl/app.py:300\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m   args \u001b[38;5;241m=\u001b[39m _run_init(\n\u001b[1;32m    301\u001b[0m       sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;28;01mif\u001b[39;00m argv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m argv,\n\u001b[1;32m    302\u001b[0m       flags_parser,\n\u001b[1;32m    303\u001b[0m   )\n\u001b[1;32m    304\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m _init_callbacks:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/absl/app.py:369\u001b[0m, in \u001b[0;36m_run_init\u001b[0;34m(argv, flags_parser)\u001b[0m\n\u001b[1;32m    368\u001b[0m logging\u001b[38;5;241m.\u001b[39muse_absl_handler()\n\u001b[0;32m--> 369\u001b[0m args \u001b[38;5;241m=\u001b[39m _register_and_parse_flags_with_usage(\n\u001b[1;32m    370\u001b[0m     argv\u001b[38;5;241m=\u001b[39margv,\n\u001b[1;32m    371\u001b[0m     flags_parser\u001b[38;5;241m=\u001b[39mflags_parser,\n\u001b[1;32m    372\u001b[0m )\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m faulthandler:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/absl/app.py:216\u001b[0m, in \u001b[0;36m_register_and_parse_flags_with_usage\u001b[0;34m(argv, flags_parser)\u001b[0m\n\u001b[1;32m    215\u001b[0m original_argv \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;28;01mif\u001b[39;00m argv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m argv\n\u001b[0;32m--> 216\u001b[0m args_to_main \u001b[38;5;241m=\u001b[39m flags_parser(original_argv)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m FLAGS\u001b[38;5;241m.\u001b[39mis_parsed():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/absl/app.py:166\u001b[0m, in \u001b[0;36mparse_flags_with_usage\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    165\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPass --helpshort or --helpfull to see help on flags.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mInteractiveTB\u001b[38;5;241m.\u001b[39mget_exception_only(etype,\n\u001b[1;32m   2122\u001b[0m                                                      value))\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[1;32m    569\u001b[0m             etype,\n\u001b[1;32m    570\u001b[0m             evalue,\n\u001b[1;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[1;32m    573\u001b[0m             context,\n\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FormattedTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[1;32m   1436\u001b[0m     \u001b[38;5;28mself\u001b[39m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[1;32m   1437\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VerboseTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[1;32m   1327\u001b[0m         \u001b[38;5;28mself\u001b[39m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[1;32m   1328\u001b[0m     )\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1171\u001b[0m ):\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m   1174\u001b[0m                                                            tb_offset)\n\u001b[1;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1064\u001b[0m )\n\u001b[1;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(cf\u001b[38;5;241m.\u001b[39mtb_frame)\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "if 'kwai' in socket.gethostname():\n",
    "    #  flags.DEFINE_string('name', 'kuaishou-GRU4REC', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('name', 'kuaishou-CASER', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('name', 'kuaishou-DIN', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('name', 'kuaishou-DIEN', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('name', 'kuaishou-SLIREC', 'Experiment name.')\n",
    "    flags.DEFINE_string('name', 'kuaishou-SURGE', 'Experiment name.')\n",
    "\n",
    "    flags.DEFINE_string('dataset', 'kuaishou', 'Dataset name.')\n",
    "    flags.DEFINE_integer('val_num_ngs', 1, 'Number of negative instances with a positiver instance for validation.')\n",
    "    flags.DEFINE_integer('test_num_ngs', 1, 'Number of negative instances with a positive instance for testing.')\n",
    "    flags.DEFINE_integer('batch_size', 500, 'Batch size.')\n",
    "    #  flags.DEFINE_string('model', 'GRU4REC', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('model', 'CASER', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('model', 'DIN', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('model', 'DIEN', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('model', 'SLIREC', 'Experiment name.')\n",
    "    flags.DEFINE_string('model', 'SURGE', 'Experiment name.')\n",
    "    flags.DEFINE_float('embed_l2', 1e-6, 'L2 regulation for embeddings.')\n",
    "    flags.DEFINE_float('layer_l2', 1e-6, 'L2 regulation for layers.')\n",
    "    flags.DEFINE_integer('gpu_id', 0, 'GPU ID.')\n",
    "    flags.DEFINE_integer('contrastive_length_threshold', 10, 'Minimum sequence length value to apply contrastive loss.')\n",
    "    flags.DEFINE_integer('contrastive_recent_k', 5, 'Use the most recent k embeddings to compute short-term proxy.')\n",
    "else:\n",
    "    #  flags.DEFINE_string('name', 'taobao-GRU4REC', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('name', 'taobao-CASER', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('name', 'taobao-DIN', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('name', 'taobao-DIEN', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('name', 'taobao-SLIREC', 'Experiment name.')\n",
    "    flags.DEFINE_string('name', 'taobao-SURGE', 'Experiment name.')\n",
    "    #  flags.DEFINE_string('dataset', 'taobao', 'Dataset name.')\n",
    "    flags.DEFINE_string('dataset', 'taobao_global', 'Dataset name.')\n",
    "    #  flags.DEFINE_string('dataset', 'amazon', 'Dataset name.')\n",
    "    flags.DEFINE_integer('gpu_id', 1, 'GPU ID.')\n",
    "    flags.DEFINE_integer('val_num_ngs', 4, 'Number of negative instances with a positiver instance for validation.')\n",
    "    flags.DEFINE_integer('test_num_ngs', 99, 'Number of negative instances with a positive instance for testing.')\n",
    "    flags.DEFINE_integer('batch_size', 500, 'Batch size.')\n",
    "    #  flags.DEFINE_integer('port', 8123, 'Port for visdom.') # for step visual \n",
    "    flags.DEFINE_integer('port', 8023, 'Port for visdom.') # for epoch visual\n",
    "    #  flags.DEFINE_string('model', 'GRU4REC', 'Model name.')\n",
    "    #  flags.DEFINE_string('model', 'CASER', 'Model name.')\n",
    "    #  flags.DEFINE_string('model', 'DIN', 'Model name.')\n",
    "    #  flags.DEFINE_string('model', 'DIEN', 'Model name.')\n",
    "    #  flags.DEFINE_string('model', 'DCASER', 'Model name.')\n",
    "    #  flags.DEFINE_string('model', 'SLIREC', 'Model name.')\n",
    "    flags.DEFINE_string('model', 'SURGE', 'Model name.')\n",
    "    flags.DEFINE_float('embed_l2', 1e-6, 'L2 regulation for embeddings.')\n",
    "    flags.DEFINE_float('layer_l2', 1e-6, 'L2 regulation for layers.')\n",
    "    flags.DEFINE_integer('contrastive_length_threshold', 5, 'Minimum sequence length value to apply contrastive loss.')\n",
    "    flags.DEFINE_integer('contrastive_recent_k', 3, 'Use the most recent k embeddings to compute short-term proxy.')\n",
    "\n",
    "flags.DEFINE_boolean('amp_time_unit', True, 'Whether to amplify unit for time stamp.')\n",
    "flags.DEFINE_boolean('only_test', False, 'Only test and do not train.')\n",
    "flags.DEFINE_boolean('test_dropout', False, 'Whether to dropout during evaluation.')\n",
    "flags.DEFINE_boolean('write_prediction_to_file', False, 'Whether to write prediction to file.')\n",
    "flags.DEFINE_boolean('test_counterfactual', False, 'Whether to test with counterfactual data.')\n",
    "flags.DEFINE_string('test_counterfactual_mode', 'shuffle', 'Mode for counterfactual evaluation, could be original, shuffle or recent.')\n",
    "flags.DEFINE_integer('counterfactual_recent_k', 10, 'Use recent k interactions to predict the target item.')\n",
    "flags.DEFINE_boolean('pretrain', False, 'Whether to use pretrain and finetune.')\n",
    "#  flags.DEFINE_boolean('finetune', True, 'Whether to use pretrain and finetune.')\n",
    "#  flags.DEFINE_string('finetune_path', '/data/changjianxin/ls-recommenders/saves/GCN/gat-uii_last_pretrain/pretrain/', 'Save path.')\n",
    "flags.DEFINE_string('finetune_path', '', 'Save path.')\n",
    "flags.DEFINE_boolean('vector_alpha', False, 'Whether to use vector alpha for long short term fusion.')\n",
    "flags.DEFINE_boolean('manual_alpha', False, 'Whether to use predefined alpha for long short term fusion.')\n",
    "flags.DEFINE_float('manual_alpha_value', 0.5, 'Predifined alpha value for long short term fusion.')\n",
    "flags.DEFINE_boolean('interest_evolve', True, 'Whether to use a GRU to model interest evolution.')\n",
    "flags.DEFINE_boolean('predict_long_short', True, 'Predict whether the next interaction is driven by long-term interest or short-term interest.')\n",
    "flags.DEFINE_enum('single_part', 'no', ['no', 'long', 'short'], 'Whether to use only long, only short or both.')\n",
    "flags.DEFINE_integer('is_clip_norm', 1, 'Whether to clip gradient norm.')\n",
    "flags.DEFINE_boolean('use_complex_attention', True, 'Whether to use complex attention like DIN.')\n",
    "flags.DEFINE_boolean('use_time4lstm', True, 'Whether to use Time4LSTMCell proposed by SLIREC.')\n",
    "flags.DEFINE_integer('epochs', 100, 'Number of epochs.')\n",
    "flags.DEFINE_integer('early_stop', 2, 'Patience for early stop.')\n",
    "flags.DEFINE_integer('pretrain_epochs', 10, 'Number of pretrain epochs.')\n",
    "flags.DEFINE_integer('finetune_epochs', 100, 'Number of finetune epochs.')\n",
    "flags.DEFINE_string('data_path', os.path.join(\"..\", \"..\", \"tests\", \"resources\", \"deeprec\", \"sequential\"), 'Data file path.')\n",
    "if socket.gethostname() == 'rl3':\n",
    "    #  flags.DEFINE_string('save_path', '/data/changjianxin/ls-recommenders/saves/', 'Save path.')  #  for step visual\n",
    "    flags.DEFINE_string('save_path', '/data/changjianxin/ls-recommenders/saves_epoch/', 'Save path.')  #  for epoch visual\n",
    "else:\n",
    "    flags.DEFINE_string('save_path', '../../saves/', 'Save path.')\n",
    "    #  flags.DEFINE_string('save_path', '../../saves_step/', 'Save path.')\n",
    "flags.DEFINE_integer('train_num_ngs', 4, 'Number of negative instances with a positive instance for training.')\n",
    "flags.DEFINE_float('sample_rate', 1.0, 'Fraction of samples for training and testing.')\n",
    "flags.DEFINE_float('attn_loss_weight', 0.001, 'Loss weight for supervised attention.')\n",
    "flags.DEFINE_float('discrepancy_loss_weight', 0.01, 'Loss weight for discrepancy between long and short term user embedding.')\n",
    "flags.DEFINE_float('contrastive_loss_weight', 0.1, 'Loss weight for contrastive of long and short intention.')\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Learning rate.')\n",
    "flags.DEFINE_integer('show_step', 500, 'Step for showing metrics.')\n",
    "flags.DEFINE_string('visual_type', 'epoch', '') #  for epoch visual\n",
    "#  flags.DEFINE_string('visual_type', 'step', 'Step for drawing metrics.') #  for step visual\n",
    "flags.DEFINE_integer('visual_step', 50, 'Step for drawing metrics.')\n",
    "flags.DEFINE_string('no_visual_host', 'kwai', '')\n",
    "flags.DEFINE_boolean('enable_mail_service', False, 'Whether to e-mail yourself after each run.')\n",
    "\n",
    "\n",
    "def get_model(flags_obj, model_path, summary_path, pretrain_path, finetune_path, user_vocab, item_vocab, cate_vocab, train_num_ngs, data_path):\n",
    "\n",
    "    EPOCHS = flags_obj.epochs\n",
    "    BATCH_SIZE = flags_obj.batch_size\n",
    "    RANDOM_SEED = None  # Set None for non-deterministic result\n",
    "\n",
    "    flags_obj.amp_time_unit = flags_obj.amp_time_unit if flags_obj.model == 'DANCE' else False\n",
    "\n",
    "    if flags_obj.dataset == 'kuaishou':\n",
    "        pairwise_metrics = ['mean_mrr', 'ndcg@1;2']\n",
    "        weighted_metrics = ['wauc']\n",
    "        max_seq_length = 250\n",
    "        time_unit = 'ms' if not flags_obj.amp_time_unit else 's'\n",
    "    elif flags_obj.dataset in ['taobao_global', 'yelp_global']:\n",
    "        pairwise_metrics = ['mean_mrr', 'ndcg@2;4;6', 'hit@2;4;6']\n",
    "        weighted_metrics = ['wauc']\n",
    "        max_seq_length = 50\n",
    "        time_unit = 's' if not flags_obj.amp_time_unit else 'amp'\n",
    "    elif flags_obj.dataset == 'kuaishou_open':\n",
    "        pairwise_metrics = ['mean_mrr', 'ndcg@2;4;6', 'hit@2;4;6']\n",
    "        weighted_metrics = ['wauc']\n",
    "        max_seq_length = 200\n",
    "        time_unit = 'ms' if not flags_obj.amp_time_unit else 's'\n",
    "    else:\n",
    "        pairwise_metrics = ['mean_mrr', 'ndcg@2;4;6', 'hit@2;4;6', 'group_auc']\n",
    "        weighted_metrics = ['wauc']\n",
    "        max_seq_length = 50\n",
    "        time_unit = 's'\n",
    "\n",
    "    if not flags_obj.test_counterfactual:\n",
    "        if flags_obj.model in ['SLIREC', 'SASLIREC', 'DANCE']:\n",
    "        #  if flags_obj.model in ['SLIREC', 'SASLIREC', 'DANCE', 'SURGE']:\n",
    "            input_creator = SASequentialIterator\n",
    "        else:\n",
    "            input_creator = SequentialIterator\n",
    "    else:\n",
    "        if flags_obj.test_counterfactual_mode == 'original':\n",
    "            input_creator = SASequentialIterator\n",
    "        elif flags_obj.test_counterfactual_mode == 'recent':\n",
    "            input_creator = RecentSASequentialIterator\n",
    "        elif flags_obj.test_counterfactual_mode == 'shuffle':\n",
    "            input_creator = ShuffleSASequentialIterator\n",
    "\n",
    "    if flags_obj.single_part != 'no':\n",
    "        flags_obj.manual_alpha = True\n",
    "        if flags_obj.single_part == 'long':\n",
    "            flags_obj.manual_alpha_value = 1.0\n",
    "        else:\n",
    "            flags_obj.manual_alpha_value = 0.0\n",
    "    elif flags_obj.manual_alpha:\n",
    "        if flags_obj.manual_alpha_value == 1.0:\n",
    "            flags_obj.single_part = 'long'\n",
    "        elif flags_obj.manual_alpha_value == 0.0:\n",
    "            flags_obj.single_part = 'short'\n",
    "\n",
    "    #SliRec\n",
    "    \n",
    "    # SURGE\n",
    "    if flags_obj.model == 'SURGE':\n",
    "        yaml_file = '../../reco_utils/recommender/deeprec/config/gcn.yaml'\n",
    "        hparams = prepare_hparams(yaml_file, \n",
    "                                embed_l2=flags_obj.embed_l2, \n",
    "                                layer_l2=flags_obj.layer_l2, \n",
    "                                learning_rate=flags_obj.learning_rate, \n",
    "                                epochs=EPOCHS,\n",
    "                                EARLY_STOP=flags_obj.early_stop,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                show_step=flags_obj.show_step,\n",
    "                                visual_step=flags_obj.visual_step,\n",
    "                                visual_type=flags_obj.visual_type,\n",
    "                                MODEL_DIR=model_path,\n",
    "                                SUMMARIES_DIR=summary_path,\n",
    "                                PRETRAIN_DIR=pretrain_path,\n",
    "                                FINETUNE_DIR=finetune_path,\n",
    "                                user_vocab=user_vocab,\n",
    "                                item_vocab=item_vocab,\n",
    "                                cate_vocab=cate_vocab,\n",
    "                                need_sample=True,\n",
    "                                train_num_ngs=train_num_ngs, # provides the number of negative instances for each positive instance for loss computation.\n",
    "                                max_seq_length=max_seq_length, \n",
    "                                hidden_size=40,\n",
    "                                train_dir=os.path.join(data_path, r'train_data'),\n",
    "                                graph_dir=os.path.join(data_path, 'graphs'),\n",
    "                                pairwise_metrics=pairwise_metrics,\n",
    "                                weighted_metrics=weighted_metrics,\n",
    "                    )\n",
    "        model = SURGEModel(hparams, input_creator, seed=RANDOM_SEED)\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    flags_obj = FLAGS\n",
    "\n",
    "    setproctitle.setproctitle('{}@changjianxin'.format(flags_obj.name))\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(flags_obj.gpu_id)\n",
    "\n",
    "    print(\"System version: {}\".format(sys.version))\n",
    "    print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "    if flags_obj.enable_mail_service:\n",
    "        mail_master = MailMaster(flags_obj)\n",
    "\n",
    "    print('start experiment')\n",
    "\n",
    "    data_path = os.path.join(flags_obj.data_path, flags_obj.dataset)\n",
    "    if flags_obj.dataset == 'amazon':\n",
    "        reviews_name = 'reviews_Movies_and_TV_5.json'\n",
    "        meta_name = 'meta_Movies_and_TV.json'\n",
    "    elif flags_obj.dataset == 'yelp':\n",
    "        reviews_name = 'yelp_academic_dataset_review.json'\n",
    "        meta_name = 'yelp_academic_dataset_business.json'\n",
    "    elif flags_obj.dataset == 'taobao':\n",
    "        reviews_name = 'UserBehavior.csv'\n",
    "        meta_name = ''\n",
    "        strong_behavior_name = 'UserBuy.csv'\n",
    "    elif flags_obj.dataset == 'yelp_global':\n",
    "        reviews_name = 'yelp_academic_dataset_review.json'\n",
    "        meta_name = 'yelp_academic_dataset_business.json'\n",
    "    elif flags_obj.dataset == 'taobao_global':\n",
    "        reviews_name = 'UserBehavior.csv'\n",
    "        meta_name = ''\n",
    "        strong_behavior_name = 'UserBuy.csv'\n",
    "    elif flags_obj.dataset == 'kuaishou':\n",
    "        reviews_name = 'kuaishou.csv'\n",
    "        meta_name = ''\n",
    "    elif flags_obj.dataset == 'kuaishou_open':\n",
    "        reviews_name = 'dataset.pkl'\n",
    "        meta_name = 'visual64_select.npy'\n",
    "        category_name = 'kuaishou_open_business_recommenders.csv'\n",
    "\n",
    "    # for test\n",
    "    train_file = os.path.join(data_path, r'train_data')\n",
    "    valid_file = os.path.join(data_path, r'valid_data')\n",
    "    test_file = os.path.join(data_path, r'test_data')\n",
    "    user_vocab = os.path.join(data_path, r'user_vocab.pkl')\n",
    "    item_vocab = os.path.join(data_path, r'item_vocab.pkl')\n",
    "    cate_vocab = os.path.join(data_path, r'category_vocab.pkl')\n",
    "    output_file = os.path.join(data_path, r'output.txt')\n",
    "\n",
    "    reviews_file = os.path.join(data_path, reviews_name)\n",
    "    meta_file = os.path.join(data_path, meta_name)\n",
    "    train_num_ngs = flags_obj.train_num_ngs\n",
    "    valid_num_ngs = flags_obj.val_num_ngs\n",
    "    test_num_ngs = flags_obj.test_num_ngs\n",
    "    sample_rate = flags_obj.sample_rate\n",
    "\n",
    "    input_files = [reviews_file, meta_file, train_file, valid_file, test_file, user_vocab, item_vocab, cate_vocab]\n",
    "\n",
    "    if not os.path.exists(train_file):\n",
    "        data_preprocessing(*input_files, sample_rate=sample_rate, valid_num_ngs=valid_num_ngs, test_num_ngs=test_num_ngs, dataset=flags_obj.dataset)\n",
    "    if not os.path.exists(test_file+'_group1'):\n",
    "        if flags_obj.dataset == 'kuaishou':\n",
    "            split_length = [50, 100, 150, 200]\n",
    "        elif flags_obj.dataset == 'taobao_global':\n",
    "            split_length = [10, 20, 30, 40]\n",
    "        group_sequence(test_file=test_file, split_length=split_length)\n",
    "\n",
    "    if flags_obj.dataset in ['taobao', 'taobao_global', 'kuaishou_open']:\n",
    "        if flags_obj.dataset in ['taobao', 'taobao_global']:\n",
    "            strong_last_test_file = os.path.join(data_path, r'strong_last_test_data')\n",
    "            strong_first_test_file = os.path.join(data_path, r'strong_first_test_data')\n",
    "            weak_test_file = os.path.join(data_path, r'weak_test_data')\n",
    "            strong_last_vocab = os.path.join(data_path, r'strong_last_vocab.pkl')\n",
    "            strong_first_vocab = os.path.join(data_path, r'strong_first_vocab.pkl')\n",
    "            strong_file = os.path.join(data_path, strong_behavior_name)\n",
    "            if not os.path.exists(strong_last_test_file) or not os.path.exists(strong_first_test_file):\n",
    "                raw_data = (strong_last_test_file, strong_first_test_file, weak_test_file, strong_last_vocab, strong_first_vocab)\n",
    "                strong_data_preprocessing(raw_data, test_file, strong_file, user_vocab, item_vocab, \n",
    "                                            test_num_ngs=test_num_ngs, dataset=flags_obj.dataset)\n",
    "        elif flags_obj.dataset == 'kuaishou_open':\n",
    "            strong_like_last_test_file = os.path.join(data_path, r'strong_like_last_test_data')\n",
    "            strong_like_first_test_file = os.path.join(data_path, r'strong_like_first_test_data')\n",
    "            strong_follow_last_test_file = os.path.join(data_path, r'strong_follow_last_test_data')\n",
    "            strong_follow_first_test_file = os.path.join(data_path, r'strong_follow_first_test_data')\n",
    "            weak_test_file = os.path.join(data_path, r'weak_test_data')\n",
    "            strong_like_last_vocab = os.path.join(data_path, r'strong_like_last_vocab.pkl')\n",
    "            strong_like_first_vocab = os.path.join(data_path, r'strong_like_first_vocab.pkl')\n",
    "            strong_follow_last_vocab = os.path.join(data_path, r'strong_follow_last_vocab.pkl')\n",
    "            strong_follow_first_vocab = os.path.join(data_path, r'strong_follow_first_vocab.pkl')\n",
    "            strong_file = os.path.join(data_path, reviews_name)\n",
    "            category_file = os.path.join(data_path, category_name)\n",
    "            if not os.path.exists(strong_like_last_test_file) or not os.path.exists(strong_like_first_test_file) \\\n",
    "                or not os.path.exists(strong_follow_last_test_file) or not os.path.exists(strong_follow_first_test_file):\n",
    "                raw_data = (strong_like_last_test_file, strong_like_first_test_file, strong_follow_last_test_file, strong_follow_first_test_file, weak_test_file, \n",
    "                             strong_like_last_vocab, strong_like_first_vocab, strong_follow_last_vocab, strong_follow_first_vocab)\n",
    "                strong_data_preprocessing(raw_data, test_file, strong_file, user_vocab, item_vocab, \n",
    "                                            test_num_ngs=test_num_ngs, dataset=flags_obj.dataset, category_file=category_file)\n",
    "\n",
    "\n",
    "    save_path = os.path.join(flags_obj.save_path, flags_obj.model, flags_obj.name)\n",
    "    model_path = os.path.join(save_path, \"model/\")\n",
    "    summary_path = os.path.join(save_path, \"summary/\")\n",
    "    pretrain_path = os.path.join(save_path, \"pretrain/\")\n",
    "    #  finetune_path = os.path.join(flags_obj.save_path, 'GCN/gat-uii_last_pretrain/' ,\"pretrain/\")\n",
    "    finetune_path = flags_obj.finetune_path\n",
    "\n",
    "    model = get_model(flags_obj, model_path, summary_path, pretrain_path, finetune_path, user_vocab, item_vocab, cate_vocab, train_num_ngs, data_path)\n",
    "\n",
    "    if flags_obj.test_counterfactual:\n",
    "        ckpt_path = tf.train.latest_checkpoint(model_path)\n",
    "        model.load_model(ckpt_path)\n",
    "        calc_mean_alpha = False\n",
    "        if flags_obj.model in ['SLIREC', 'DANCE']:\n",
    "            calc_mean_alpha = True\n",
    "        print('weak interaction:')\n",
    "        res_weak = model.run_weighted_eval(weak_test_file, num_ngs=test_num_ngs, calc_mean_alpha=calc_mean_alpha) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "        print(res_weak)\n",
    "\n",
    "        if flags_obj.dataset in ['taobao', 'taobao_global']:\n",
    "            print('strong last interaction:')\n",
    "            res_strong_last = model.run_weighted_eval(strong_last_test_file, num_ngs=test_num_ngs, calc_mean_alpha=calc_mean_alpha) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "            print(res_strong_last)\n",
    "            print('strong first interaction:')\n",
    "            res_strong_first = model.run_weighted_eval(strong_first_test_file, num_ngs=test_num_ngs, calc_mean_alpha=calc_mean_alpha) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "            print(res_strong_first)\n",
    "\n",
    "            if flags_obj.enable_mail_service:\n",
    "                mail_master.send_mail(flags_obj, 'counterfactual', [res_strong_last, res_strong_first, res_weak])\n",
    "\n",
    "        elif flags_obj.dataset == 'kuaishou_open':\n",
    "            print('strong like last interaction:')\n",
    "            res_strong_like_last = model.run_weighted_eval(strong_like_last_test_file, num_ngs=test_num_ngs, calc_mean_alpha=calc_mean_alpha) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "            print(res_strong_like_last)\n",
    "            print('strong like first interaction:')\n",
    "            res_strong_like_first = model.run_weighted_eval(strong_like_first_test_file, num_ngs=test_num_ngs, calc_mean_alpha=calc_mean_alpha) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "            print(res_strong_like_first)\n",
    "            print('strong follow last interaction:')\n",
    "            res_strong_follow_last = model.run_weighted_eval(strong_follow_last_test_file, num_ngs=test_num_ngs, calc_mean_alpha=calc_mean_alpha) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "            print(res_strong_follow_last)\n",
    "            print('strong follow first interaction:')\n",
    "            res_strong_follow_first = model.run_weighted_eval(strong_follow_first_test_file, num_ngs=test_num_ngs, calc_mean_alpha=calc_mean_alpha) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "            print(res_strong_follow_first)\n",
    "\n",
    "            if flags_obj.enable_mail_service:\n",
    "                mail_master.send_mail(flags_obj, 'counterfactual', [res_strong_like_last, res_strong_like_first, res_strong_follow_last, res_strong_follow_first, res_weak])\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "    if flags_obj.only_test:\n",
    "        ckpt_path = tf.train.latest_checkpoint(model_path)\n",
    "        model.load_model(ckpt_path)\n",
    "        res_syn = model.run_weighted_eval(test_file, num_ngs=test_num_ngs) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "        print(flags_obj.name)\n",
    "        print(res_syn)\n",
    "\n",
    "        #  for g in [1,2,3,4,5]:\n",
    "            #  res_syn_group = model.run_weighted_eval(test_file+'_group'+str(g), num_ngs=test_num_ngs)\n",
    "            #  print(flags_obj.name+'_group'+str(g))\n",
    "            #  print(res_syn_group)\n",
    "\n",
    "        if flags_obj.enable_mail_service:\n",
    "            mail_master.send_mail(flags_obj, 'test', res)\n",
    "\n",
    "        return\n",
    "\n",
    "    #  if flags_obj.pretrain:\n",
    "        #  ckpt_path = tf.train.latest_checkpoint(pretrain_path)\n",
    "        #  model.load_model(ckpt_path)\n",
    "\n",
    "        #  variables = tf.contrib.framework.get_variables_to_restore()\n",
    "        #  variables = tf.contrib.framework..get_variables_to_restore()\n",
    "        #  print(variables)\n",
    "        #  variables_to_resotre = [v for v in variables if v.name.split('/')[-1] in ['item_embedding','user_embedding']]\n",
    "        #  print(variables_to_resotre)\n",
    "        #  saver_emb = tf.compat.v1.train.Saver(variables_to_resotre)\n",
    "        #  saver_emb = tf.compat.v1.train.Saver(model.item_lookup)\n",
    "        #  saver_emb.restore(self.sess, ckpt_path)\n",
    "        #  chkp.print_tensors_in_checkpoint_file(ckpt_path, tensor_name='', all_tensors=True)\n",
    "        #  chkp.print_tensors_in_checkpoint_file(ckpt_path, tensor_name='sequential/embedding/item_embedding', all_tensors=False)\n",
    "        #  chkp.print_tensors_in_checkpoint_file(ckpt_path, tensor_name='sequential/embedding/user_embedding', all_tensors=False)\n",
    "\n",
    "        #  self.item_lookup = tf.concat([self.item_lookup, self.cate_embedding], axis=1)\n",
    "        #  self.user_lookup = tf.concat([self.item_lookup, self.cate_embedding], axis=1)\n",
    "        #  import pdb; pdb.set_trace()\n",
    "\n",
    "\n",
    "    if flags_obj.no_visual_host not in socket.gethostname():\n",
    "        vm = VizManager(flags_obj)\n",
    "        vm.show_basic_info(flags_obj)\n",
    "    else:\n",
    "        vm = None\n",
    "    #  visual_path = summary_path\n",
    "    visual_path = os.path.join(save_path, \"metrics/\")\n",
    "    tb = SummaryWriter(log_dir=visual_path, comment='tb')\n",
    "\n",
    "    #  print(model.run_weighted_eval(test_file, num_ngs=test_num_ngs)) # test_num_ngs is the number of negative lines after each positive line in your test_file\n",
    "\n",
    "    if flags_obj.dataset in ['kuaishou', 'kuaishou_open', 'taobao_global', 'yelp_global']:\n",
    "        eval_metric = 'wauc'\n",
    "    else:\n",
    "        eval_metric = 'group_auc'\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = model.fit(train_file, valid_file, valid_num_ngs=valid_num_ngs, eval_metric=eval_metric, vm=vm, tb=tb, pretrain=flags_obj.pretrain) \n",
    "    # valid_num_ngs is the number of negative lines after each positive line in your valid_file \n",
    "    # we will evaluate the performance of model on valid_file every epoch\n",
    "    end_time = time.time()\n",
    "    cost_time = end_time - start_time\n",
    "    print('Time cost for training is {0:.2f} mins'.format((cost_time)/60.0))\n",
    "\n",
    "    ckpt_path = tf.train.latest_checkpoint(model_path)\n",
    "    model.load_model(ckpt_path)\n",
    "    res_syn = model.run_weighted_eval(test_file, num_ngs=test_num_ngs)\n",
    "    print(flags_obj.name)\n",
    "    print(res_syn)\n",
    "\n",
    "    for g in [1,2,3,4,5]:\n",
    "        res_syn_group = model.run_weighted_eval(test_file+'_group'+str(g), num_ngs=test_num_ngs)\n",
    "        print(flags_obj.name+'_group'+str(g))\n",
    "        print(res_syn_group)\n",
    "\n",
    "    if flags_obj.no_visual_host not in socket.gethostname():\n",
    "        vm.show_test_info()\n",
    "        vm.show_result(res_syn)\n",
    "\n",
    "    tb.close()\n",
    "\n",
    "    if flags_obj.enable_mail_service:\n",
    "        mail_master.send_mail(flags_obj, 'train', res)\n",
    "\n",
    "    if flags_obj.write_prediction_to_file:\n",
    "        model = model.predict(test_file, output_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca5d65af-87b7-4777-bfe0-98d05b56f7dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reco_utils.recommender.deeprec.models.surge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter \u001b[38;5;28;01mas\u001b[39;00m SummaryWriter\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreco_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecommender\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeeprec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msurge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SURGEModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreco_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecommender\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeeprec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SASequentialIterator\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreco_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecommender\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeeprec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdkn_iterator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DKNTextIterator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'reco_utils.recommender.deeprec.models.surge'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define dataset paths and files\n",
    "data_path = \"\"\n",
    "reviews_file = os.path.join(data_path, \"reviews_Movies_and_TV_5.json\")\n",
    "meta_file = os.path.join(data_path, \"meta_Movies_and_TV.json\")\n",
    "train_file = os.path.join(data_path, \"train_data\")\n",
    "valid_file = os.path.join(data_path, \"valid_data\")\n",
    "test_file = os.path.join(data_path, \"test_data\")\n",
    "user_vocab = os.path.join(data_path, \"user_vocab.pkl\")\n",
    "item_vocab = os.path.join(data_path, \"item_vocab.pkl\")\n",
    "cate_vocab = os.path.join(data_path, \"category_vocab.pkl\")\n",
    "\n",
    "# Define model parameters\n",
    "model_name = \"amazon-SURGE\"\n",
    "save_path = \"surge-files\"\n",
    "model_path = os.path.join(save_path, \"model\")\n",
    "summary_path = os.path.join(save_path, \"summary\")\n",
    "pretrain_path = os.path.join(save_path, \"pretrain\")\n",
    "finetune_path = \"\"\n",
    "train_num_ngs = 4\n",
    "epochs = 100\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "visual_type = \"epoch\"\n",
    "\n",
    "# Prepare model hyperparameters\n",
    "yaml_file = \"../../reco_utils/recommender/deeprec/config/gcn.yaml\"\n",
    "hparams = prepare_hparams(\n",
    "    yaml_file,\n",
    "    embed_l2=1e-6,\n",
    "    layer_l2=1e-6,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    show_step=500,\n",
    "    visual_step=50,\n",
    "    visual_type=visual_type,\n",
    "    MODEL_DIR=model_path,\n",
    "    SUMMARIES_DIR=summary_path,\n",
    "    PRETRAIN_DIR=pretrain_path,\n",
    "    FINETUNE_DIR=finetune_path,\n",
    "    user_vocab=user_vocab,\n",
    "    item_vocab=item_vocab,\n",
    "    cate_vocab=cate_vocab,\n",
    "    need_sample=True,\n",
    "    train_num_ngs=train_num_ngs,\n",
    "    max_seq_length=250,\n",
    "    hidden_size=40,\n",
    "    train_dir=os.path.join(data_path, \"train_data\"),\n",
    "    graph_dir=os.path.join(data_path, \"graphs\"),\n",
    "    pairwise_metrics=[\"mean_mrr\", \"ndcg@1;2\"],\n",
    "    weighted_metrics=[\"wauc\"],\n",
    ")\n",
    "\n",
    "# Create SURGE model instance\n",
    "model = SURGEModel(hparams, SASequentialIterator, seed=None)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "model = model.fit(train_file, valid_file, valid_num_ngs=1, eval_metric=\"wauc\")\n",
    "end_time = time.time()\n",
    "print(\"Training time:\", end_time - start_time)\n",
    "\n",
    "# Evaluate the model\n",
    "res = model.run_weighted_eval(test_file, num_ngs=1)\n",
    "print(\"Evaluation results:\", res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81ff21-c2d2-4897-b271-1ea8b732190a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
